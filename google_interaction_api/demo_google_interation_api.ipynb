{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Gemini Interactions API Demo\n",
        "\n",
        "This notebook demonstrates the key features of the Google Gemini Interactions API, a unified interface for interacting with Gemini models and agents that simplifies state management, tool orchestration, and long-running tasks.\n",
        "\n",
        "**Note:** The Interactions API is in Beta. Features and schemas are subject to breaking changes.\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs/interactions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install python-dotenv to load .env files\n",
        "! pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, we need to install the `google-genai` package (version 1.55.0 or later) and set up the client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-genai==1.56.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (1.56.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai==1.56.0) (2.45.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (4.13.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-genai==1.56.0) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.56.0) (3.10)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai==1.56.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai==1.56.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai==1.56.0) (4.7.2)\n",
            "Requirement already satisfied: certifi in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.56.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.56.0) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai==1.56.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.56.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.56.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.56.0) (0.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.56.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.56.0) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/karu/.pyenv/versions/3.12.8/envs/env_notebooks1/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai==1.56.0) (0.6.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install the google-genai package (uncomment if not already installed)\n",
        "! pip install google-genai==1.56.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "import os\n",
        "\n",
        "# Initialize the client\n",
        "# The API key can be set via environment variable GEMINI_API_KEY in .env file\n",
        "# Get your Gemini API key from https://aistudio.google.com/app/apikey\n",
        "# or passed directly: client = genai.Client(api_key=\"YOUR_API_KEY\")\n",
        "client = genai.Client()\n",
        "\n",
        "print(\"Client initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Interactions\n",
        "\n",
        "The simplest way to interact with the model is by providing a text prompt. The `input` can be a string, a list containing content objects, or a list of turns with roles and content objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Response:\n",
            "The capital of India is **New Delhi**.\n",
            "\n",
            "--- Interaction Object Structure ---\n",
            "Interaction ID: v1_Chd1aXBMYVkyN0M3dUR6N0lQblpDbXlRWRIXdWlwTGFZMjdDN3VEejdJUG5aQ215UVk\n",
            "Status: completed\n",
            "Number of outputs: 2\n"
          ]
        }
      ],
      "source": [
        "# Create a basic interaction with a simple text prompt\n",
        "interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Capital of India.\"\n",
        ")\n",
        "\n",
        "# Display the output text\n",
        "print(\"Model Response:\")\n",
        "print(interaction.outputs[-1].text)\n",
        "print(\"\\n--- Interaction Object Structure ---\")\n",
        "print(f\"Interaction ID: {interaction.id}\")\n",
        "print(f\"Status: {interaction.status}\")\n",
        "print(f\"Number of outputs: {len(interaction.outputs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stateful Conversations\n",
        "\n",
        "You can build **multi-turn conversations** (unlimited turns) by referencing a previous interaction using the `previous_interaction_id` parameter. This allows the server to retrieve the full context, saving you from having to resend the entire chat history. Each new turn should reference the **most recent interaction's ID** to maintain the conversation chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hi, my name is Karthik. I like to play cricket.And i like watching movies.\n",
            "Model: Hi Karthik! It’s great to meet you.\n",
            "\n",
            "You have two excellent hobbies! Since you enjoy **cricket**, do you prefer batting, bowling, or are you an all-rounder? Also, do you have a favorite international team or player you look up to?\n",
            "\n",
            "As for **movies**, what’s your favorite genre? Are you into big action blockbusters, thrillers, or maybe some classic drama? \n",
            "\n",
            "I'd love to hear more about your favorites!\n",
            "User: What is my name?\n",
            "Model: Your name is **Karthik**! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "# First turn: Create initial interaction\n",
        "input_text = \"Hi, my name is Karthik. I like to play cricket.And i like watching movies.\"\n",
        "interaction1 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text\n",
        ")\n",
        "\n",
        "print(f\"User: {input_text}\")\n",
        "print(f\"Model: {interaction1.outputs[-1].text}\")\n",
        "\n",
        "# Turn 2\n",
        "input_text = \"What is my name?\"\n",
        "interaction2 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text,\n",
        "    previous_interaction_id=interaction1.id\n",
        ")\n",
        "\n",
        "print(f\"User: {input_text}\")\n",
        "print(f\"Model: {interaction2.outputs[-1].text}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multi-Turn Conversations\n",
        "\n",
        "Stateful conversations support **unlimited turns**. You can continue chaining conversations by always referencing the most recent interaction's ID. Each turn builds upon the full conversation history maintained on the server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TURN 1\n",
            "============================================================\n",
            "User: I'm planning a trip to Japan. Can you suggest 3 cities I should visit? Answer in very short sentence.\n",
            "Model: Visit Tokyo for city life, Kyoto for ancient temples, and Osaka for street food.\n",
            "Interaction ID: v1_ChdfaXBMYWZxSkVQQ2h6N0lQLXNxbGtRYxIXX2lwTGFmcUpFUENoejdJUC1zcWxrUWM\n",
            "\n",
            "============================================================\n",
            "TURN 2\n",
            "============================================================\n",
            "User: What's the best time of year to visit the first city you mentioned? Answer in very short sentence.\n",
            "Model: Spring and autumn are the best times.\n",
            "Interaction ID: v1_ChdfaXBMYWZxSkVQQ2h6N0lQLXNxbGtRYxIXQUN0TGFmT2dMS0hWejdJUHk4YUUwQVE\n",
            "\n",
            "============================================================\n",
            "TURN 3\n",
            "============================================================\n",
            "User: Can you recommend a traditional dish I should try in that city? Answer in very short sentence.\n",
            "Model: Try authentic Nigiri-zushi.\n",
            "Interaction ID: v1_ChdfaXBMYWZxSkVQQ2h6N0lQLXNxbGtRYxIXQWl0TGFacV9NTzZnejdJUHdhNjQyQVU\n",
            "\n",
            "============================================================\n",
            "TURN 4\n",
            "============================================================\n",
            "User: What about the second city you mentioned? What's special about it? Answer in short very short sentence.\n",
            "Model: It's famous for ancient temples and shrines.\n",
            "Interaction ID: v1_ChdfaXBMYWZxSkVQQ2h6N0lQLXNxbGtRYxIXQkN0TGFZNldOOU9zejdJUG1zU3F5QVE\n",
            "\n",
            "✅ Multi-turn conversation completed!\n",
            "The model maintained context across all 4 turns without needing to resend history.\n"
          ]
        }
      ],
      "source": [
        "# Multi-turn conversation example (3+ turns)\n",
        "# Each turn references the previous interaction to maintain context\n",
        "\n",
        "# Turn 1: Initial interaction\n",
        "input_text = \"I'm planning a trip to Japan. Can you suggest 3 cities I should visit? Answer in very short sentence.\"\n",
        "turn1 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TURN 1\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"User: {input_text}\")\n",
        "print(f\"Model: {turn1.outputs[-1].text}\")  # Truncate for display\n",
        "print(f\"Interaction ID: {turn1.id}\\n\")\n",
        "\n",
        "# Turn 2: Continue conversation using turn1's ID\n",
        "input_text = \"What's the best time of year to visit the first city you mentioned? Answer in very short sentence.\"\n",
        "turn2 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text,\n",
        "    previous_interaction_id=turn1.id\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TURN 2\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"User: {input_text}\")\n",
        "# print(f\"Model: {turn2.outputs[-1].text[:200]}...\")\n",
        "print(f\"Model: {turn2.outputs[-1].text}\")\n",
        "print(f\"Interaction ID: {turn2.id}\\n\")\n",
        "\n",
        "# Turn 3: Continue using turn2's ID (the most recent interaction)\n",
        "input_text = \"Can you recommend a traditional dish I should try in that city? Answer in very short sentence.\"\n",
        "turn3 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text,\n",
        "    previous_interaction_id=turn2.id\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TURN 3\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"User: {input_text}\")\n",
        "print(f\"Model: {turn3.outputs[-1].text}\")\n",
        "print(f\"Interaction ID: {turn3.id}\\n\")\n",
        "\n",
        "# Turn 4: Continue using turn3's ID\n",
        "input_text = \"What about the second city you mentioned? What's special about it? Answer in short very short sentence.\" \n",
        "turn4 = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=input_text,\n",
        "    previous_interaction_id=turn3.id\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TURN 4\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"User: {input_text}\")\n",
        "print(f\"Model: {turn4.outputs[-1].text}\")\n",
        "print(f\"Interaction ID: {turn4.id}\\n\")\n",
        "\n",
        "print(\"✅ Multi-turn conversation completed!\")\n",
        "print(f\"The model maintained context across all {4} turns without needing to resend history.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Retrieving Past Interactions\n",
        "\n",
        "You can retrieve previous interactions using the interaction `id` to access past turns of the conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved Interaction:\n",
            "ID: v1_ChdfaXBMYWZxSkVQQ2h6N0lQLXNxbGtRYxIXQkN0TGFZNldOOU9zejdJUG1zU3F5QVE\n",
            "Status: completed\n",
            "Model: gemini-3-flash-preview\n",
            "\n",
            "Outputs:\n",
            "  Output 1: ThoughtContent(type='thought', signature='EpkFCpYFAXLI2nyb8h4uN+pM5dAx81FPzJuCJdAo9AknXQH8AkYUDaHNLQhf3bw9mo1MlxNLTdQskR7HS5mwNV/AVBXDuH3P3/LN5uzpx84dLTTP3PJnS4Blh5Syv34iaBXHJtQC9XAYJDthurhV4VkPH6Ei3I6j5eDZZGXY5tVPhdlqzXiWYP9+JScmf8UBqp1ZJNjglXaLBS5IpC5eUpeQk1asJzVpTTG0A9bF9EFHj/1w/7fVcHqcViGL4LRyUN8qhrjpYyQ1UaKqcoWBDMmW8xw/F34FSjwV7jOk4agZVOO5d6r2hwx4Dq6+NM0waKB2GPY83IXCXrRrA6xdRXXys79Zr8b/bW1kNxT4n7MgjP3+/WV/xnBaM8UjXFDrQUojyWpGOzlgf0nHPedtvTZEkwQttSigZrVzVHvnbbib6UeurLpDAR+DZyg8bubs5DKjJihaGotVh90SYwbDzrHj1f78fYTyrREFJmYjX9Qtt+6XUZIIc32EXQwmVFNzdPM0SzooVYZa6aTSBOutFJKLCQXalrXiMhmPVe+Yi0othjD/3QF1oVYpFNmY9lDyOh56zgi1PVZfqcc8Qno1LdHWquV9PZetV0DfdX1AJBccEHhziAiVTA1/9n5l48GJTEGnLnZO5ts363XVKOa0+W/e9mqjsceYf2QAtUyBbLeUZlRVmdwJlbiOG1TQPQz2Knh9YcRtBkxGfP14LoLkfu+xKEvJ9a6Wb2nPpiVlj/IAPdGLjXihifo39vZY0JmHMQsRx8ENA+U6EFl9zMJBkLJkItROrVOmNEtI9Fa3aTa3bWr11HhdRljOQ6R3lZEXSQONbrBOlIq9tOEFEPAQjhyMHMQ4t3jC86pA/xdQhMsJa1CpePi/dLLJJms=', summary=None)\n",
            "  Output 2: It's famous for ancient temples and shrines.\n"
          ]
        }
      ],
      "source": [
        "# Retrieve a previous interaction using its ID\n",
        "# Replace with an actual interaction ID from a previous run if needed\n",
        "previous_interaction = client.interactions.get(turn4.id)\n",
        "\n",
        "print(\"Retrieved Interaction:\")\n",
        "print(f\"ID: {previous_interaction.id}\")\n",
        "print(f\"Status: {previous_interaction.status}\")\n",
        "print(f\"Model: {previous_interaction.model}\")\n",
        "print(f\"\\nOutputs:\")\n",
        "for i, output in enumerate(previous_interaction.outputs):\n",
        "    print(f\"  Output {i+1}: {output.text if hasattr(output, 'text') else output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stateless Conversations\n",
        "\n",
        "You can also manage conversation history manually on the client side without using server-side state management. This gives you full control over the conversation flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Turn (Stateless):\n",
            "User: What are the three largest cities in Spain?\n",
            "Model: The three largest cities in Spain by population are:\n",
            "\n",
            "1.  **Madrid** (approx. 3.3 million)\n",
            "2.  **Barcelona** (approx. 1.6 million)\n",
            "3.  **Valencia** (approx. 800,000)\n",
            "\n",
            "Second Turn (Stateless):\n",
            "User: What is the most famous landmark in the second one?\n",
            "Model: The most famous landmark in **Barcelona** is the **Sagrada Família**.\n",
            "\n",
            "Designed by the architect Antoni Gaudí, this massive Catholic basilica has been under construction since 1882 and is world-renowned for its unique organic shapes, towering spires, and intricate facades. It is a UNESCO World Heritage site and the most visited monument in Spain.\n",
            "\n",
            "Full conversation history maintained on client side.\n"
          ]
        }
      ],
      "source": [
        "# Build conversation history manually\n",
        "conversation_history = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are the three largest cities in Spain?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# First interaction with the conversation history\n",
        "interaction1_stateless = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=conversation_history\n",
        ")\n",
        "\n",
        "print(\"First Turn (Stateless):\")\n",
        "print(f\"User: {conversation_history[0]['content']}\")\n",
        "print(f\"Model: {interaction1_stateless.outputs[-1].text}\\n\")\n",
        "\n",
        "# Append model response to history\n",
        "conversation_history.append({\n",
        "    \"role\": \"model\", \n",
        "    \"content\": interaction1_stateless.outputs\n",
        "})\n",
        "\n",
        "# Add next user message\n",
        "conversation_history.append({\n",
        "    \"role\": \"user\", \n",
        "    \"content\": \"What is the most famous landmark in the second one?\"\n",
        "})\n",
        "\n",
        "# Second interaction with full history\n",
        "interaction2_stateless = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=conversation_history\n",
        ")\n",
        "\n",
        "print(\"Second Turn (Stateless):\")\n",
        "print(f\"User: {conversation_history[-1]['content']}\")\n",
        "print(f\"Model: {interaction2_stateless.outputs[-1].text}\")\n",
        "print(f\"\\nFull conversation history maintained on client side.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Background Execution\n",
        "\n",
        "The `background=True` parameter allows you to run interactions asynchronously, which is useful for long-running tasks or when you want to continue working while the interaction processes. Note that `background=True` requires `store=True` (which is the default).\n",
        "\n",
        "When using background execution:\n",
        "- The interaction is created immediately and returns with a status\n",
        "- You can check the status periodically using `client.interactions.get(interaction_id)`\n",
        "- The interaction continues processing on the server side\n",
        "- Once complete, you can retrieve the final results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Create an interaction with background=True\n",
        "# This will run asynchronously on the server\n",
        "background_interaction = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"Write a detailed explanation of how neural networks work, including the concepts of forward propagation, backpropagation, and gradient descent.\",\n",
        "    background=True  # Run in background\n",
        ")\n",
        "\n",
        "print(f\"Background interaction created!\")\n",
        "print(f\"Interaction ID: {background_interaction.id}\")\n",
        "print(f\"Initial Status: {background_interaction.status}\")\n",
        "print(f\"\\nThe interaction is now processing in the background...\")\n",
        "print(f\"You can continue with other tasks while it runs.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Poll for completion - check the status periodically\n",
        "# In a real application, you might want to do other work here\n",
        "max_wait_time = 60  # Maximum time to wait (seconds)\n",
        "check_interval = 2  # Check every 2 seconds\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Polling for completion...\")\n",
        "while True:\n",
        "    # Retrieve the current status\n",
        "    current_interaction = client.interactions.get(background_interaction.id)\n",
        "    status = current_interaction.status\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Status: {status} (elapsed: {elapsed_time:.1f}s)\")\n",
        "    \n",
        "    # Check if completed\n",
        "    if status == \"COMPLETE\":\n",
        "        print(\"\\n✅ Interaction completed!\")\n",
        "        print(f\"\\nFinal Response:\")\n",
        "        print(current_interaction.outputs[-1].text)\n",
        "        break\n",
        "    elif status in [\"FAILED\", \"CANCELLED\"]:\n",
        "        print(f\"\\n❌ Interaction ended with status: {status}\")\n",
        "        break\n",
        "    elif elapsed_time > max_wait_time:\n",
        "        print(f\"\\n⏱️  Timeout reached. Interaction may still be processing.\")\n",
        "        print(f\"Final Status: {status}\")\n",
        "        print(f\"You can check again later using: client.interactions.get('{background_interaction.id}')\")\n",
        "        break\n",
        "    \n",
        "    time.sleep(check_interval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Best Practices and Important Notes\n",
        "\n",
        "### Data Storage and Retention\n",
        "\n",
        "- **By default**, all Interaction objects are stored (`store=true`) to enable:\n",
        "  - Server-side state management (with `previous_interaction_id`)\n",
        "  - Background execution (using `background=true`)\n",
        "  - Observability purposes\n",
        "\n",
        "- **Retention periods:**\n",
        "  - **Paid Tier**: Interactions are retained for **55 days**\n",
        "  - **Free Tier**: Interactions are retained for **1 day**\n",
        "\n",
        "- To opt out of storage, set `store=false` in your request. However, note that:\n",
        "  - `store=false` is incompatible with `background=true`\n",
        "  - `store=false` prevents using `previous_interaction_id` for subsequent turns\n",
        "\n",
        "### Cache Hit Rate\n",
        "\n",
        "Using `previous_interaction_id` to continue conversations allows the system to more easily utilize implicit caching for the conversation history, which improves performance and reduces costs.\n",
        "\n",
        "### Mixing Interactions\n",
        "\n",
        "You have the flexibility to mix and match Agent and Model interactions within a conversation. For instance, you can use a specialized agent (like the Deep Research agent) for initial data collection, and then use a standard Gemini model for follow-up tasks such as summarizing or reformatting, linking these steps with the `previous_interaction_id`.\n",
        "\n",
        "### Supported Models\n",
        "\n",
        "- `gemini-2.5-pro`\n",
        "- `gemini-2.5-flash`\n",
        "- `gemini-2.5-flash-lite`\n",
        "- `gemini-3-pro-preview`\n",
        "- `gemini-3-flash-preview`\n",
        "\n",
        "### Limitations \n",
        "\n",
        "**As of:** January 2025  \n",
        "**Source:** [Official API Documentation](https://ai.google.dev/gemini-api/docs/interactions)\n",
        "\n",
        "- **Beta status**: The Interactions API is in beta/preview. Features and schemas may change.\n",
        "- **Unsupported features** (coming soon):\n",
        "  - Grounding with Google Maps\n",
        "  - Computer Use\n",
        "- **Output ordering**: Content ordering for built-in tools may sometimes be incorrect\n",
        "- **Tool combinations**: Combining MCP, Function Call, and Built-in tools is not yet supported\n",
        "\n",
        "> **Note:** Limitations are subject to change as the API evolves. Please refer to the [official documentation](https://ai.google.dev/gemini-api/docs/interactions) for the most up-to-date information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Creating an interaction with store=false to opt out of storage\n",
        "# Note: This prevents using previous_interaction_id in future calls\n",
        "\n",
        "interaction_no_store = client.interactions.create(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    input=\"This interaction will not be stored.\",\n",
        "    store=False\n",
        ")\n",
        "\n",
        "print(f\"Interaction created with store=False\")\n",
        "print(f\"ID: {interaction_no_store.id}\")\n",
        "print(f\"Response: {interaction_no_store.outputs[-1].text}\")\n",
        "print(\"\\nNote: This interaction cannot be retrieved later and cannot be used with previous_interaction_id\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_notebooks1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
